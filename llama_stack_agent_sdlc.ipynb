{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sternlieb/AI/blob/main/llama_stack_agent_sdlc.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Llama Stack Setup"
      ],
      "metadata": {
        "id": "VE_0Pb4Y0SK3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZjdbV1D9sICP",
        "outputId": "38754a1a-42c0-4529-81b7-691774654124",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "bubblewrap is already the newest version (0.6.1-1ubuntu0.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 22 not upgraded.\n",
            "Requirement already satisfied: uv in /usr/local/lib/python3.11/dist-packages (0.6.3)\n",
            "\u001b[2mUsing Python 3.11.11 environment at: /usr\u001b[0m\n",
            "\u001b[2mAudited \u001b[1m1 package\u001b[0m \u001b[2min 114ms\u001b[0m\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!apt-get install -y bubblewrap\n",
        "!pip install uv\n",
        "!uv pip install llama-stack --system"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ['FIREWORKS_API_KEY'] = userdata.get('FIREWORKS_API_KEY')\n",
        "os.environ['TAVILY_SEARCH_API_KEY'] = userdata.get('TAVILY_SEARCH_API_KEY')\n",
        "\n",
        "!llama stack build --template fireworks --image-type venv --image-name __system__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "kUiIzY26sNjY",
        "outputId": "801204ee-237e-4c25-e257-efc58fca26b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing dependencies in system Python environment\r\n",
            "\u001b[2mUsing Python 3.11.11 environment at: /usr\u001b[0m\r\n",
            "\u001b[2mAudited \u001b[1m1 package\u001b[0m \u001b[2min 82ms\u001b[0m\u001b[0m\r\n",
            "Installing pip dependencies\r\n",
            "\u001b[2mUsing Python 3.11.11 environment at: /usr\u001b[0m\r\n",
            "\u001b[2mAudited \u001b[1m32 packages\u001b[0m \u001b[2min 83ms\u001b[0m\u001b[0m\r\n",
            "sentence-transformers --no-deps\n",
            "\u001b[2mUsing Python 3.11.11 environment at: /usr\u001b[0m\n",
            "\u001b[2mAudited \u001b[1m1 package\u001b[0m \u001b[2min 83ms\u001b[0m\u001b[0m\n",
            "torch torchvision --index-url https://download.pytorch.org/whl/cpu\n",
            "\u001b[2mUsing Python 3.11.11 environment at: /usr\u001b[0m\n",
            "\u001b[2mAudited \u001b[1m2 packages\u001b[0m \u001b[2min 68ms\u001b[0m\u001b[0m\n",
            "\u001b[32mBuild Successful!\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from rich.pretty import pprint\n",
        "\n",
        "from llama_stack.distribution.library_client import LlamaStackAsLibraryClient\n",
        "\n",
        "from llama_stack_client.lib.agents.agent import Agent\n",
        "from llama_stack_client.lib.agents.event_logger import EventLogger\n",
        "from llama_stack_client.lib.agents.react.agent import ReActAgent, ReActOutput\n",
        "from llama_stack_client.lib.agents.react.prompts import DEFAULT_REACT_AGENT_SYSTEM_PROMPT_TEMPLATE\n",
        "\n",
        "from llama_stack_client.types.agent_create_params import AgentConfig\n"
      ],
      "metadata": {
        "id": "-23lp6YNwH1d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initialize the Llama Stack Client"
      ],
      "metadata": {
        "id": "tDaQHFxo0rDz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "client = LlamaStackAsLibraryClient(\n",
        "    # distribution name ( this one uses fireworks.ai as the underlying provider for the /inference apis )\n",
        "    \"fireworks\",\n",
        "    provider_data = {\"tavily_search_api_key\": os.environ['TAVILY_SEARCH_API_KEY']}\n",
        ")\n",
        "\n",
        "client.initialize()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "DJ3lfnKCsQV1",
        "outputId": "d658308b-ee3a-4361-9cfe-0587ac76f181"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Using config \u001b[34mfireworks\u001b[0m:\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Using config <span style=\"color: #000080; text-decoration-color: #000080\">fireworks</span>:\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "apis:\n",
              "- agents\n",
              "- datasetio\n",
              "- eval\n",
              "- inference\n",
              "- safety\n",
              "- scoring\n",
              "- telemetry\n",
              "- tool_runtime\n",
              "- vector_io\n",
              "benchmarks: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m\n",
              "container_image: null\n",
              "datasets: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m\n",
              "image_name: fireworks\n",
              "metadata_store:\n",
              "  db_path: \u001b[35m/root/.llama/distributions/fireworks/\u001b[0m\u001b[95mregistry.db\u001b[0m\n",
              "  namespace: null\n",
              "  type: sqlite\n",
              "models:\n",
              "- metadata: \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n",
              "  model_id: meta-llama/Llama-\u001b[1;36m3.1\u001b[0m-8B-Instruct\n",
              "  model_type: !!python/object/apply:llama_stack.apis.models.models.ModelType\n",
              "  - llm\n",
              "  provider_id: fireworks\n",
              "  provider_model_id: accounts/fireworks/models/llama-v3p1-8b-instruct\n",
              "- metadata: \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n",
              "  model_id: meta-llama/Llama-\u001b[1;36m3.1\u001b[0m-70B-Instruct\n",
              "  model_type: !!python/object/apply:llama_stack.apis.models.models.ModelType\n",
              "  - llm\n",
              "  provider_id: fireworks\n",
              "  provider_model_id: accounts/fireworks/models/llama-v3p1-70b-instruct\n",
              "- metadata: \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n",
              "  model_id: meta-llama/Llama-\u001b[1;36m3.1\u001b[0m-405B-Instruct-FP8\n",
              "  model_type: !!python/object/apply:llama_stack.apis.models.models.ModelType\n",
              "  - llm\n",
              "  provider_id: fireworks\n",
              "  provider_model_id: accounts/fireworks/models/llama-v3p1-405b-instruct\n",
              "- metadata: \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n",
              "  model_id: meta-llama/Llama-\u001b[1;36m3.2\u001b[0m-1B-Instruct\n",
              "  model_type: !!python/object/apply:llama_stack.apis.models.models.ModelType\n",
              "  - llm\n",
              "  provider_id: fireworks\n",
              "  provider_model_id: accounts/fireworks/models/llama-v3p2-1b-instruct\n",
              "- metadata: \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n",
              "  model_id: meta-llama/Llama-\u001b[1;36m3.2\u001b[0m-3B-Instruct\n",
              "  model_type: !!python/object/apply:llama_stack.apis.models.models.ModelType\n",
              "  - llm\n",
              "  provider_id: fireworks\n",
              "  provider_model_id: accounts/fireworks/models/llama-v3p2-3b-instruct\n",
              "- metadata: \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n",
              "  model_id: meta-llama/Llama-\u001b[1;36m3.2\u001b[0m-11B-Vision-Instruct\n",
              "  model_type: !!python/object/apply:llama_stack.apis.models.models.ModelType\n",
              "  - llm\n",
              "  provider_id: fireworks\n",
              "  provider_model_id: accounts/fireworks/models/llama-v3p2-11b-vision-instruct\n",
              "- metadata: \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n",
              "  model_id: meta-llama/Llama-\u001b[1;36m3.2\u001b[0m-90B-Vision-Instruct\n",
              "  model_type: !!python/object/apply:llama_stack.apis.models.models.ModelType\n",
              "  - llm\n",
              "  provider_id: fireworks\n",
              "  provider_model_id: accounts/fireworks/models/llama-v3p2-90b-vision-instruct\n",
              "- metadata: \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n",
              "  model_id: meta-llama/Llama-\u001b[1;36m3.3\u001b[0m-70B-Instruct\n",
              "  model_type: !!python/object/apply:llama_stack.apis.models.models.ModelType\n",
              "  - llm\n",
              "  provider_id: fireworks\n",
              "  provider_model_id: accounts/fireworks/models/llama-v3p3-70b-instruct\n",
              "- metadata: \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n",
              "  model_id: meta-llama/Llama-Guard-\u001b[1;36m3\u001b[0m-8B\n",
              "  model_type: !!python/object/apply:llama_stack.apis.models.models.ModelType\n",
              "  - llm\n",
              "  provider_id: fireworks\n",
              "  provider_model_id: accounts/fireworks/models/llama-guard-\u001b[1;36m3\u001b[0m-8b\n",
              "- metadata: \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n",
              "  model_id: meta-llama/Llama-Guard-\u001b[1;36m3\u001b[0m-11B-Vision\n",
              "  model_type: !!python/object/apply:llama_stack.apis.models.models.ModelType\n",
              "  - llm\n",
              "  provider_id: fireworks\n",
              "  provider_model_id: accounts/fireworks/models/llama-guard-\u001b[1;36m3\u001b[0m-11b-vision\n",
              "- metadata:\n",
              "    context_length: \u001b[1;36m8192\u001b[0m\n",
              "    embedding_dimension: \u001b[1;36m768\u001b[0m\n",
              "  model_id: nomic-ai/nomic-embed-text-v1.\u001b[1;36m5\u001b[0m\n",
              "  model_type: !!python/object/apply:llama_stack.apis.models.models.ModelType\n",
              "  - embedding\n",
              "  provider_id: fireworks\n",
              "  provider_model_id: nomic-ai/nomic-embed-text-v1.\u001b[1;36m5\u001b[0m\n",
              "- metadata:\n",
              "    embedding_dimension: \u001b[1;36m384\u001b[0m\n",
              "  model_id: all-MiniLM-L6-v2\n",
              "  model_type: !!python/object/apply:llama_stack.apis.models.models.ModelType\n",
              "  - embedding\n",
              "  provider_id: sentence-transformers\n",
              "  provider_model_id: null\n",
              "providers:\n",
              "  agents:\n",
              "  - config:\n",
              "      persistence_store:\n",
              "        db_path: \u001b[35m/root/.llama/distributions/fireworks/\u001b[0m\u001b[95magents_store.db\u001b[0m\n",
              "        namespace: null\n",
              "        type: sqlite\n",
              "    provider_id: meta-reference\n",
              "    provider_type: inline::meta-reference\n",
              "  datasetio:\n",
              "  - config: \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n",
              "    provider_id: huggingface\n",
              "    provider_type: remote::huggingface\n",
              "  - config: \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n",
              "    provider_id: localfs\n",
              "    provider_type: inline::localfs\n",
              "  eval:\n",
              "  - config: \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n",
              "    provider_id: meta-reference\n",
              "    provider_type: inline::meta-reference\n",
              "  inference:\n",
              "  - config:\n",
              "      api_key: \u001b[32m'********'\u001b[0m\n",
              "      url: \u001b[4;94mhttps://api.fireworks.ai/inference/v1\u001b[0m\n",
              "    provider_id: fireworks\n",
              "    provider_type: remot\u001b[1;92me::f\u001b[0mireworks\n",
              "  - config: \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n",
              "    provider_id: sentence-transformers\n",
              "    provider_type: inline::sentence-transformers\n",
              "  safety:\n",
              "  - config: \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n",
              "    provider_id: llama-guard\n",
              "    provider_type: inline::llama-guard\n",
              "  scoring:\n",
              "  - config: \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n",
              "    provider_id: basic\n",
              "    provider_type: inlin\u001b[1;92me::ba\u001b[0msic\n",
              "  - config: \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n",
              "    provider_id: llm-as-judge\n",
              "    provider_type: inline::llm-as-judge\n",
              "  - config:\n",
              "      openai_api_key: \u001b[32m'********'\u001b[0m\n",
              "    provider_id: braintrust\n",
              "    provider_type: inlin\u001b[1;92me::b\u001b[0mraintrust\n",
              "  telemetry:\n",
              "  - config:\n",
              "      service_name: llama-stack\n",
              "      sinks: sqlite\n",
              "      sqlite_db_path: \u001b[35m/root/.llama/distributions/fireworks/\u001b[0m\u001b[95mtrace_store.db\u001b[0m\n",
              "    provider_id: meta-reference\n",
              "    provider_type: inline::meta-reference\n",
              "  tool_runtime:\n",
              "  - config:\n",
              "      api_key: \u001b[32m'********'\u001b[0m\n",
              "      max_results: \u001b[1;36m3\u001b[0m\n",
              "    provider_id: brave-search\n",
              "    provider_type: remot\u001b[1;92me::b\u001b[0mrave-search\n",
              "  - config:\n",
              "      api_key: \u001b[32m'********'\u001b[0m\n",
              "      max_results: \u001b[1;36m3\u001b[0m\n",
              "    provider_id: tavily-search\n",
              "    provider_type: remote::tavily-search\n",
              "  - config: \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n",
              "    provider_id: code-interpreter\n",
              "    provider_type: inlin\u001b[1;92me::c\u001b[0mode-interpreter\n",
              "  - config: \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n",
              "    provider_id: rag-runtime\n",
              "    provider_type: inline::rag-runtime\n",
              "  - config: \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n",
              "    provider_id: model-context-protocol\n",
              "    provider_type: remote::model-context-protocol\n",
              "  vector_io:\n",
              "  - config:\n",
              "      kvstore:\n",
              "        db_path: \u001b[35m/root/.llama/distributions/fireworks/\u001b[0m\u001b[95mfaiss_store.db\u001b[0m\n",
              "        namespace: null\n",
              "        type: sqlite\n",
              "    provider_id: faiss\n",
              "    provider_type: inlin\u001b[1;92me::fa\u001b[0miss\n",
              "scoring_fns: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m\n",
              "server:\n",
              "  port: \u001b[1;36m8321\u001b[0m\n",
              "  tls_certfile: null\n",
              "  tls_keyfile: null\n",
              "shields:\n",
              "- params: null\n",
              "  provider_id: null\n",
              "  provider_shield_id: null\n",
              "  shield_id: meta-llama/Llama-Guard-\u001b[1;36m3\u001b[0m-8B\n",
              "tool_groups:\n",
              "- args: null\n",
              "  mcp_endpoint: null\n",
              "  provider_id: tavily-search\n",
              "  toolgroup_id: builtin::websearch\n",
              "- args: null\n",
              "  mcp_endpoint: null\n",
              "  provider_id: rag-runtime\n",
              "  toolgroup_id: builtin::rag\n",
              "- args: null\n",
              "  mcp_endpoint: null\n",
              "  provider_id: code-interpreter\n",
              "  toolgroup_id: builtin::code_interpreter\n",
              "vector_dbs: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m\n",
              "version: \u001b[32m'2'\u001b[0m\n",
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">apis:\n",
              "- agents\n",
              "- datasetio\n",
              "- eval\n",
              "- inference\n",
              "- safety\n",
              "- scoring\n",
              "- telemetry\n",
              "- tool_runtime\n",
              "- vector_io\n",
              "benchmarks: <span style=\"font-weight: bold\">[]</span>\n",
              "container_image: null\n",
              "datasets: <span style=\"font-weight: bold\">[]</span>\n",
              "image_name: fireworks\n",
              "metadata_store:\n",
              "  db_path: <span style=\"color: #800080; text-decoration-color: #800080\">/root/.llama/distributions/fireworks/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">registry.db</span>\n",
              "  namespace: null\n",
              "  type: sqlite\n",
              "models:\n",
              "- metadata: <span style=\"font-weight: bold\">{}</span>\n",
              "  model_id: meta-llama/Llama-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3.1</span>-8B-Instruct\n",
              "  model_type: !!python/object/apply:llama_stack.apis.models.models.ModelType\n",
              "  - llm\n",
              "  provider_id: fireworks\n",
              "  provider_model_id: accounts/fireworks/models/llama-v3p1-8b-instruct\n",
              "- metadata: <span style=\"font-weight: bold\">{}</span>\n",
              "  model_id: meta-llama/Llama-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3.1</span>-70B-Instruct\n",
              "  model_type: !!python/object/apply:llama_stack.apis.models.models.ModelType\n",
              "  - llm\n",
              "  provider_id: fireworks\n",
              "  provider_model_id: accounts/fireworks/models/llama-v3p1-70b-instruct\n",
              "- metadata: <span style=\"font-weight: bold\">{}</span>\n",
              "  model_id: meta-llama/Llama-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3.1</span>-405B-Instruct-FP8\n",
              "  model_type: !!python/object/apply:llama_stack.apis.models.models.ModelType\n",
              "  - llm\n",
              "  provider_id: fireworks\n",
              "  provider_model_id: accounts/fireworks/models/llama-v3p1-405b-instruct\n",
              "- metadata: <span style=\"font-weight: bold\">{}</span>\n",
              "  model_id: meta-llama/Llama-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3.2</span>-1B-Instruct\n",
              "  model_type: !!python/object/apply:llama_stack.apis.models.models.ModelType\n",
              "  - llm\n",
              "  provider_id: fireworks\n",
              "  provider_model_id: accounts/fireworks/models/llama-v3p2-1b-instruct\n",
              "- metadata: <span style=\"font-weight: bold\">{}</span>\n",
              "  model_id: meta-llama/Llama-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3.2</span>-3B-Instruct\n",
              "  model_type: !!python/object/apply:llama_stack.apis.models.models.ModelType\n",
              "  - llm\n",
              "  provider_id: fireworks\n",
              "  provider_model_id: accounts/fireworks/models/llama-v3p2-3b-instruct\n",
              "- metadata: <span style=\"font-weight: bold\">{}</span>\n",
              "  model_id: meta-llama/Llama-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3.2</span>-11B-Vision-Instruct\n",
              "  model_type: !!python/object/apply:llama_stack.apis.models.models.ModelType\n",
              "  - llm\n",
              "  provider_id: fireworks\n",
              "  provider_model_id: accounts/fireworks/models/llama-v3p2-11b-vision-instruct\n",
              "- metadata: <span style=\"font-weight: bold\">{}</span>\n",
              "  model_id: meta-llama/Llama-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3.2</span>-90B-Vision-Instruct\n",
              "  model_type: !!python/object/apply:llama_stack.apis.models.models.ModelType\n",
              "  - llm\n",
              "  provider_id: fireworks\n",
              "  provider_model_id: accounts/fireworks/models/llama-v3p2-90b-vision-instruct\n",
              "- metadata: <span style=\"font-weight: bold\">{}</span>\n",
              "  model_id: meta-llama/Llama-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3.3</span>-70B-Instruct\n",
              "  model_type: !!python/object/apply:llama_stack.apis.models.models.ModelType\n",
              "  - llm\n",
              "  provider_id: fireworks\n",
              "  provider_model_id: accounts/fireworks/models/llama-v3p3-70b-instruct\n",
              "- metadata: <span style=\"font-weight: bold\">{}</span>\n",
              "  model_id: meta-llama/Llama-Guard-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>-8B\n",
              "  model_type: !!python/object/apply:llama_stack.apis.models.models.ModelType\n",
              "  - llm\n",
              "  provider_id: fireworks\n",
              "  provider_model_id: accounts/fireworks/models/llama-guard-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>-8b\n",
              "- metadata: <span style=\"font-weight: bold\">{}</span>\n",
              "  model_id: meta-llama/Llama-Guard-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>-11B-Vision\n",
              "  model_type: !!python/object/apply:llama_stack.apis.models.models.ModelType\n",
              "  - llm\n",
              "  provider_id: fireworks\n",
              "  provider_model_id: accounts/fireworks/models/llama-guard-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>-11b-vision\n",
              "- metadata:\n",
              "    context_length: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8192</span>\n",
              "    embedding_dimension: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">768</span>\n",
              "  model_id: nomic-ai/nomic-embed-text-v1.<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>\n",
              "  model_type: !!python/object/apply:llama_stack.apis.models.models.ModelType\n",
              "  - embedding\n",
              "  provider_id: fireworks\n",
              "  provider_model_id: nomic-ai/nomic-embed-text-v1.<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>\n",
              "- metadata:\n",
              "    embedding_dimension: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">384</span>\n",
              "  model_id: all-MiniLM-L6-v2\n",
              "  model_type: !!python/object/apply:llama_stack.apis.models.models.ModelType\n",
              "  - embedding\n",
              "  provider_id: sentence-transformers\n",
              "  provider_model_id: null\n",
              "providers:\n",
              "  agents:\n",
              "  - config:\n",
              "      persistence_store:\n",
              "        db_path: <span style=\"color: #800080; text-decoration-color: #800080\">/root/.llama/distributions/fireworks/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">agents_store.db</span>\n",
              "        namespace: null\n",
              "        type: sqlite\n",
              "    provider_id: meta-reference\n",
              "    provider_type: inline::meta-reference\n",
              "  datasetio:\n",
              "  - config: <span style=\"font-weight: bold\">{}</span>\n",
              "    provider_id: huggingface\n",
              "    provider_type: remote::huggingface\n",
              "  - config: <span style=\"font-weight: bold\">{}</span>\n",
              "    provider_id: localfs\n",
              "    provider_type: inline::localfs\n",
              "  eval:\n",
              "  - config: <span style=\"font-weight: bold\">{}</span>\n",
              "    provider_id: meta-reference\n",
              "    provider_type: inline::meta-reference\n",
              "  inference:\n",
              "  - config:\n",
              "      api_key: <span style=\"color: #008000; text-decoration-color: #008000\">'********'</span>\n",
              "      url: <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://api.fireworks.ai/inference/v1</span>\n",
              "    provider_id: fireworks\n",
              "    provider_type: remot<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">e::f</span>ireworks\n",
              "  - config: <span style=\"font-weight: bold\">{}</span>\n",
              "    provider_id: sentence-transformers\n",
              "    provider_type: inline::sentence-transformers\n",
              "  safety:\n",
              "  - config: <span style=\"font-weight: bold\">{}</span>\n",
              "    provider_id: llama-guard\n",
              "    provider_type: inline::llama-guard\n",
              "  scoring:\n",
              "  - config: <span style=\"font-weight: bold\">{}</span>\n",
              "    provider_id: basic\n",
              "    provider_type: inlin<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">e::ba</span>sic\n",
              "  - config: <span style=\"font-weight: bold\">{}</span>\n",
              "    provider_id: llm-as-judge\n",
              "    provider_type: inline::llm-as-judge\n",
              "  - config:\n",
              "      openai_api_key: <span style=\"color: #008000; text-decoration-color: #008000\">'********'</span>\n",
              "    provider_id: braintrust\n",
              "    provider_type: inlin<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">e::b</span>raintrust\n",
              "  telemetry:\n",
              "  - config:\n",
              "      service_name: llama-stack\n",
              "      sinks: sqlite\n",
              "      sqlite_db_path: <span style=\"color: #800080; text-decoration-color: #800080\">/root/.llama/distributions/fireworks/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">trace_store.db</span>\n",
              "    provider_id: meta-reference\n",
              "    provider_type: inline::meta-reference\n",
              "  tool_runtime:\n",
              "  - config:\n",
              "      api_key: <span style=\"color: #008000; text-decoration-color: #008000\">'********'</span>\n",
              "      max_results: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>\n",
              "    provider_id: brave-search\n",
              "    provider_type: remot<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">e::b</span>rave-search\n",
              "  - config:\n",
              "      api_key: <span style=\"color: #008000; text-decoration-color: #008000\">'********'</span>\n",
              "      max_results: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>\n",
              "    provider_id: tavily-search\n",
              "    provider_type: remote::tavily-search\n",
              "  - config: <span style=\"font-weight: bold\">{}</span>\n",
              "    provider_id: code-interpreter\n",
              "    provider_type: inlin<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">e::c</span>ode-interpreter\n",
              "  - config: <span style=\"font-weight: bold\">{}</span>\n",
              "    provider_id: rag-runtime\n",
              "    provider_type: inline::rag-runtime\n",
              "  - config: <span style=\"font-weight: bold\">{}</span>\n",
              "    provider_id: model-context-protocol\n",
              "    provider_type: remote::model-context-protocol\n",
              "  vector_io:\n",
              "  - config:\n",
              "      kvstore:\n",
              "        db_path: <span style=\"color: #800080; text-decoration-color: #800080\">/root/.llama/distributions/fireworks/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">faiss_store.db</span>\n",
              "        namespace: null\n",
              "        type: sqlite\n",
              "    provider_id: faiss\n",
              "    provider_type: inlin<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">e::fa</span>iss\n",
              "scoring_fns: <span style=\"font-weight: bold\">[]</span>\n",
              "server:\n",
              "  port: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8321</span>\n",
              "  tls_certfile: null\n",
              "  tls_keyfile: null\n",
              "shields:\n",
              "- params: null\n",
              "  provider_id: null\n",
              "  provider_shield_id: null\n",
              "  shield_id: meta-llama/Llama-Guard-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>-8B\n",
              "tool_groups:\n",
              "- args: null\n",
              "  mcp_endpoint: null\n",
              "  provider_id: tavily-search\n",
              "  toolgroup_id: builtin::websearch\n",
              "- args: null\n",
              "  mcp_endpoint: null\n",
              "  provider_id: rag-runtime\n",
              "  toolgroup_id: builtin::rag\n",
              "- args: null\n",
              "  mcp_endpoint: null\n",
              "  provider_id: code-interpreter\n",
              "  toolgroup_id: builtin::code_interpreter\n",
              "vector_dbs: <span style=\"font-weight: bold\">[]</span>\n",
              "version: <span style=\"color: #008000; text-decoration-color: #008000\">'2'</span>\n",
              "\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Most of the code above was required to setup the client to work on notebooks.\n",
        "With llama-stack APIs and plugin architecture you can now point the below code to any distribution (on-prem, cloud, localhost) and it should work seamlessly."
      ],
      "metadata": {
        "id": "QEskMYel-DJv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Simple Chat Completion"
      ],
      "metadata": {
        "id": "lCVDsrA7frLy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_id = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
        "response = client.inference.chat_completion(\n",
        "    model_id=model_id,\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a friendly assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": \"Write a two-sentence poem about llama.\"},\n",
        "    ],\n",
        "    stream=False,\n",
        ")\n",
        "\n",
        "print(response.completion_message.content)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nTwnQDDBfpPO",
        "outputId": "53e486f0-814f-4543-e036-70d70572799d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A gentle llama roams the land,\n",
            "With soft fur and a gentle hand.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Agents\n",
        "\n",
        "Chat completion is good to work with the llm directly.\n",
        "\n",
        "The power of LLMs can be harnessed when we can have them do more complex tasks. We also added Agent APIs to llama stack.\n",
        "\n",
        "A simple agents can be characterized as having access to\n",
        "\n",
        "- Tool Calling - Ability to call tools like search, code execution, DB lookups\n",
        "- A control loop controlled by the llm\n",
        "\n",
        "Here is a simple search agent,"
      ],
      "metadata": {
        "id": "t9OP_ID2gc3b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "config = AgentConfig(\n",
        "    model=\"meta-llama/Llama-3.3-70B-Instruct\",\n",
        "    instructions=\"You are a helpful assistant\",\n",
        "    toolgroups=[\"builtin::websearch\"],\n",
        ")\n",
        "\n",
        "agent = Agent(client, config)\n",
        "\n",
        "user_prompts = [\n",
        "    \"What are the latest news about llama-con from Meta in the last week ?\",\n",
        "]\n",
        "\n",
        "session_id = agent.create_session(\"test-session\")\n",
        "for prompt in user_prompts:\n",
        "    # print(f\"\\nUser> {prompt}\")\n",
        "    response = agent.create_turn(\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": prompt,\n",
        "            }\n",
        "        ],\n",
        "        session_id=session_id,\n",
        "        stream=False,\n",
        "    )\n",
        "    pprint(response)\n"
      ],
      "metadata": {
        "id": "8beXQdS6he-D",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "02f7d923-4559-41c0-bdfe-9e57513e3692"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;35mTurn\u001b[0m\u001b[1m(\u001b[0m\n",
              "\u001b[2;32m│   \u001b[0m\u001b[33minput_messages\u001b[0m=\u001b[1m[\u001b[0m\n",
              "\u001b[2;32m│   │   \u001b[0m\u001b[1;35mUserMessage\u001b[0m\u001b[1m(\u001b[0m\n",
              "\u001b[2;32m│   │   │   \u001b[0m\u001b[33mcontent\u001b[0m=\u001b[32m'What are the latest news about llama-con from Meta in the last week ?'\u001b[0m,\n",
              "\u001b[2;32m│   │   │   \u001b[0m\u001b[33mrole\u001b[0m=\u001b[32m'user'\u001b[0m,\n",
              "\u001b[2;32m│   │   │   \u001b[0m\u001b[33mcontext\u001b[0m=\u001b[3;35mNone\u001b[0m\n",
              "\u001b[2;32m│   │   \u001b[0m\u001b[1m)\u001b[0m\n",
              "\u001b[2;32m│   \u001b[0m\u001b[1m]\u001b[0m,\n",
              "\u001b[2;32m│   \u001b[0m\u001b[33moutput_message\u001b[0m=\u001b[1;35mCompletionMessage\u001b[0m\u001b[1m(\u001b[0m\n",
              "\u001b[2;32m│   │   \u001b[0m\u001b[33mcontent\u001b[0m=\u001b[32m\"The\u001b[0m\u001b[32m latest news about Llama-con from Meta in the last week includes the announcement of the LlamaCon conference, which will focus on AI and open-source AI developments. The conference is scheduled for April 29, 2025, and will feature updates on Meta's open-source AI model Llama, as well as new tools and APIs for developers to create apps and products. Additionally, Meta has announced that its Llama model has been downloaded 650 million times in 2024, and the company has maintained 600 million users every day during the year.\"\u001b[0m,\n",
              "\u001b[2;32m│   │   \u001b[0m\u001b[33mrole\u001b[0m=\u001b[32m'assistant'\u001b[0m,\n",
              "\u001b[2;32m│   │   \u001b[0m\u001b[33mstop_reason\u001b[0m=\u001b[32m'end_of_turn'\u001b[0m,\n",
              "\u001b[2;32m│   │   \u001b[0m\u001b[33mtool_calls\u001b[0m=\u001b[1m[\u001b[0m\u001b[1m]\u001b[0m\n",
              "\u001b[2;32m│   \u001b[0m\u001b[1m)\u001b[0m,\n",
              "\u001b[2;32m│   \u001b[0m\u001b[33msession_id\u001b[0m=\u001b[32m'098c983c-9178-4d08-ad66-767985466f01'\u001b[0m,\n",
              "\u001b[2;32m│   \u001b[0m\u001b[33mstarted_at\u001b[0m=\u001b[1;35mdatetime\u001b[0m\u001b[1;35m.datetime\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m2025\u001b[0m, \u001b[1;36m2\u001b[0m, \u001b[1;36m26\u001b[0m, \u001b[1;36m20\u001b[0m, \u001b[1;36m49\u001b[0m, \u001b[1;36m50\u001b[0m, \u001b[1;36m969864\u001b[0m\u001b[1m)\u001b[0m,\n",
              "\u001b[2;32m│   \u001b[0m\u001b[33msteps\u001b[0m=\u001b[1m[\u001b[0m\n",
              "\u001b[2;32m│   │   \u001b[0m\u001b[1;35mInferenceStep\u001b[0m\u001b[1m(\u001b[0m\n",
              "\u001b[2;32m│   │   │   \u001b[0m\u001b[33mapi_model_response\u001b[0m=\u001b[1;35mCompletionMessage\u001b[0m\u001b[1m(\u001b[0m\n",
              "\u001b[2;32m│   │   │   │   \u001b[0m\u001b[33mcontent\u001b[0m=\u001b[32m''\u001b[0m,\n",
              "\u001b[2;32m│   │   │   │   \u001b[0m\u001b[33mrole\u001b[0m=\u001b[32m'assistant'\u001b[0m,\n",
              "\u001b[2;32m│   │   │   │   \u001b[0m\u001b[33mstop_reason\u001b[0m=\u001b[32m'end_of_turn'\u001b[0m,\n",
              "\u001b[2;32m│   │   │   │   \u001b[0m\u001b[33mtool_calls\u001b[0m=\u001b[1m[\u001b[0m\n",
              "\u001b[2;32m│   │   │   │   │   \u001b[0m\u001b[1;35mToolCall\u001b[0m\u001b[1m(\u001b[0m\n",
              "\u001b[2;32m│   │   │   │   │   │   \u001b[0m\u001b[33marguments\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'query'\u001b[0m: \u001b[32m'Llama-con Meta latest news last week'\u001b[0m\u001b[1m}\u001b[0m,\n",
              "\u001b[2;32m│   │   │   │   │   │   \u001b[0m\u001b[33mcall_id\u001b[0m=\u001b[32m'367a78af-c9d7-46d8-83fd-6b96dbc43b50'\u001b[0m,\n",
              "\u001b[2;32m│   │   │   │   │   │   \u001b[0m\u001b[33mtool_name\u001b[0m=\u001b[32m'brave_search'\u001b[0m\n",
              "\u001b[2;32m│   │   │   │   │   \u001b[0m\u001b[1m)\u001b[0m\n",
              "\u001b[2;32m│   │   │   │   \u001b[0m\u001b[1m]\u001b[0m\n",
              "\u001b[2;32m│   │   │   \u001b[0m\u001b[1m)\u001b[0m,\n",
              "\u001b[2;32m│   │   │   \u001b[0m\u001b[33mstep_id\u001b[0m=\u001b[32m'99c13bb9-d57f-4547-903d-3ecef7983419'\u001b[0m,\n",
              "\u001b[2;32m│   │   │   \u001b[0m\u001b[33mstep_type\u001b[0m=\u001b[32m'inference'\u001b[0m,\n",
              "\u001b[2;32m│   │   │   \u001b[0m\u001b[33mturn_id\u001b[0m=\u001b[32m'de2227df-76e6-4549-a0ee-51bbfc6de046'\u001b[0m,\n",
              "\u001b[2;32m│   │   │   \u001b[0m\u001b[33mcompleted_at\u001b[0m=\u001b[1;35mdatetime\u001b[0m\u001b[1;35m.datetime\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m2025\u001b[0m, \u001b[1;36m2\u001b[0m, \u001b[1;36m26\u001b[0m, \u001b[1;36m20\u001b[0m, \u001b[1;36m49\u001b[0m, \u001b[1;36m51\u001b[0m, \u001b[1;36m881539\u001b[0m\u001b[1m)\u001b[0m,\n",
              "\u001b[2;32m│   │   │   \u001b[0m\u001b[33mstarted_at\u001b[0m=\u001b[1;35mdatetime\u001b[0m\u001b[1;35m.datetime\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m2025\u001b[0m, \u001b[1;36m2\u001b[0m, \u001b[1;36m26\u001b[0m, \u001b[1;36m20\u001b[0m, \u001b[1;36m49\u001b[0m, \u001b[1;36m50\u001b[0m, \u001b[1;36m972186\u001b[0m\u001b[1m)\u001b[0m\n",
              "\u001b[2;32m│   │   \u001b[0m\u001b[1m)\u001b[0m,\n",
              "\u001b[2;32m│   │   \u001b[0m\u001b[1;35mToolExecutionStep\u001b[0m\u001b[1m(\u001b[0m\n",
              "\u001b[2;32m│   │   │   \u001b[0m\u001b[33mstep_id\u001b[0m=\u001b[32m'476ebdd8-149f-4774-aad8-25f93c91f8c3'\u001b[0m,\n",
              "\u001b[2;32m│   │   │   \u001b[0m\u001b[33mstep_type\u001b[0m=\u001b[32m'tool_execution'\u001b[0m,\n",
              "\u001b[2;32m│   │   │   \u001b[0m\u001b[33mtool_calls\u001b[0m=\u001b[1m[\u001b[0m\n",
              "\u001b[2;32m│   │   │   │   \u001b[0m\u001b[1;35mToolCall\u001b[0m\u001b[1m(\u001b[0m\n",
              "\u001b[2;32m│   │   │   │   │   \u001b[0m\u001b[33marguments\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'query'\u001b[0m: \u001b[32m'Llama-con Meta latest news last week'\u001b[0m\u001b[1m}\u001b[0m,\n",
              "\u001b[2;32m│   │   │   │   │   \u001b[0m\u001b[33mcall_id\u001b[0m=\u001b[32m'367a78af-c9d7-46d8-83fd-6b96dbc43b50'\u001b[0m,\n",
              "\u001b[2;32m│   │   │   │   │   \u001b[0m\u001b[33mtool_name\u001b[0m=\u001b[32m'brave_search'\u001b[0m\n",
              "\u001b[2;32m│   │   │   │   \u001b[0m\u001b[1m)\u001b[0m\n",
              "\u001b[2;32m│   │   │   \u001b[0m\u001b[1m]\u001b[0m,\n",
              "\u001b[2;32m│   │   │   \u001b[0m\u001b[33mtool_responses\u001b[0m=\u001b[1m[\u001b[0m\n",
              "\u001b[2;32m│   │   │   │   \u001b[0m\u001b[1;35mToolResponse\u001b[0m\u001b[1m(\u001b[0m\n",
              "\u001b[2;32m│   │   │   │   │   \u001b[0m\u001b[33mcall_id\u001b[0m=\u001b[32m'367a78af-c9d7-46d8-83fd-6b96dbc43b50'\u001b[0m,\n",
              "\u001b[2;32m│   │   │   │   │   \u001b[0m\u001b[33mcontent\u001b[0m=\u001b[32m'\u001b[0m\u001b[32m{\u001b[0m\u001b[32m\"query\": \"Llama-con Meta latest news last week\", \"top_k\": \u001b[0m\u001b[32m[\u001b[0m\u001b[32m{\u001b[0m\u001b[32m\"title\": \"Meta\\'s new \\'LlamaCon\\' event will focus on AI | The Verge\", \"url\": \"https://www.theverge.com/news/614455/meta-llamacon-connect-2025-date-announcement\", \"content\": \"Meta\\\\u2019s new \\\\u2018LlamaCon\\\\u2019 event will focus on AI | The Verge Meta\\\\u2019s new \\\\u2018LlamaCon\\\\u2019 event will focus on AI Meta\\\\u2019s new \\\\u2018LlamaCon\\\\u2019 event will focus on AI Emma Roth is a news writer who covers the streaming wars, consumer tech, crypto, social media, and much more. Meta is holding a new \\\\u201cLlamaCon\\\\u201d conference where it will share updates on \\\\u201copen source AI developments\\\\u201d to help developers create apps and products, the company said on Tuesday. Meta said it will reveal more information about LlamaCon, which is named after the company\\\\u2019s open-source AI model Llama, in \\\\u201cthe coming weeks.\\\\u201d The Connect conference will feature the \\\\u201clatest and greatest in Meta Horizon updates\\\\u201d and will \\\\u201cpeel back the curtain on tomorrow\\\\u2019s tech,\\\\u201d Meta says.\", \"score\": 0.8003337, \"raw_content\": null\u001b[0m\u001b[32m}\u001b[0m\u001b[32m, \u001b[0m\u001b[32m{\u001b[0m\u001b[32m\"title\": \"Meta Announces LlamaCon, Riding the Wave of Llama Model Success\", \"url\": \"https://finance.yahoo.com/news/meta-announces-llamacon-riding-wave-110708834.html\", \"content\": \"Feb 19 - Meta Platforms \u001b[0m\u001b[32m(\u001b[0m\u001b[32mMETA, Financial\u001b[0m\u001b[32m)\u001b[0m\u001b[32m announced its upcoming developer conference, LlamaCon, scheduled for April 29, 2025, in response to the significant growth of its open-source Llama model\", \"score\": 0.78512776, \"raw_content\": null\u001b[0m\u001b[32m}\u001b[0m\u001b[32m, \u001b[0m\u001b[32m{\u001b[0m\u001b[32m\"title\": \"Meta Announces LlamaCon, Riding the Wave of Llama Model Success - GuruFocus\", \"url\": \"https://www.gurufocus.com/news/2703591/meta-announces-llamacon-riding-the-wave-of-llama-model-success\", \"content\": \"Feb 19 - Meta Platforms \u001b[0m\u001b[32m(\u001b[0m\u001b[32mMETA, Financial\u001b[0m\u001b[32m)\u001b[0m\u001b[32m announced its upcoming developer conference, LlamaCon, scheduled for April 29, 2025, in response to the significant growth of its open-source Llama model. A total of 650 million times the Llama model was downloaded in 2024; meanwhile, Meta AI maintained 600 million users every day during the year.\", \"score\": 0.783542, \"raw_content\": null\u001b[0m\u001b[32m}\u001b[0m\u001b[32m, \u001b[0m\u001b[32m{\u001b[0m\u001b[32m\"title\": \"Meta just scheduled a generative AI conference called ... - Engadget\", \"url\": \"https://www.engadget.com/ai/meta-just-scheduled-a-generative-ai-conference-called-llamacon-for-april-29-181351134.html\", \"content\": \"Meta just scheduled a generative AI conference called LlamaCon for April 29 Best in Tech View all Reviews Best Robot Vacuums Best Laptops Best Gaming Laptops Best VPN View all Buying Guides View all Gear View all Entertainment View all Tomorrow The best music streaming services Meta just scheduled a generative AI conference called LlamaCon for April 29 Meta just announced its first-ever LlamaCon, a dev conference dedicated to generative AI. The company titled the event after its family of generative AI models. Meta promises to \\\\u201cshare the latest on our open source AI developments to help developers do what they do best: build amazing apps and products.\\\\u201d Beyond that vague description, we don\\\\u2019t know much.\", \"score\": 0.7451949, \"raw_content\": null\u001b[0m\u001b[32m}\u001b[0m\u001b[32m, \u001b[0m\u001b[32m{\u001b[0m\u001b[32m\"title\": \"Meta\\'s LlamaCon 2025: What Developers Should Expect - Analytics Insight\", \"url\": \"https://www.analyticsinsight.net/ampstories/tech-news/metas-llamacon-2025-what-developers-should-expect\", \"content\": \"Meta\\'s LlamaCon 2025 will showcase cutting-edge AI advancements, including updates to Llama models for more efficient and scalable applications. Developers can expect new tools and APIs that enhance AI-driven chatbots, automation, and content creation across Meta\\'s platforms.\", \"score\": 0.71948266, \"raw_content\": null\u001b[0m\u001b[32m}\u001b[0m\u001b[32m]\u001b[0m\u001b[32m}\u001b[0m\u001b[32m'\u001b[0m,\n",
              "\u001b[2;32m│   │   │   │   │   \u001b[0m\u001b[33mtool_name\u001b[0m=\u001b[32m'brave_search'\u001b[0m,\n",
              "\u001b[2;32m│   │   │   │   │   \u001b[0m\u001b[33mmetadata\u001b[0m=\u001b[3;35mNone\u001b[0m\n",
              "\u001b[2;32m│   │   │   │   \u001b[0m\u001b[1m)\u001b[0m\n",
              "\u001b[2;32m│   │   │   \u001b[0m\u001b[1m]\u001b[0m,\n",
              "\u001b[2;32m│   │   │   \u001b[0m\u001b[33mturn_id\u001b[0m=\u001b[32m'de2227df-76e6-4549-a0ee-51bbfc6de046'\u001b[0m,\n",
              "\u001b[2;32m│   │   │   \u001b[0m\u001b[33mcompleted_at\u001b[0m=\u001b[1;35mdatetime\u001b[0m\u001b[1;35m.datetime\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m2025\u001b[0m, \u001b[1;36m2\u001b[0m, \u001b[1;36m26\u001b[0m, \u001b[1;36m20\u001b[0m, \u001b[1;36m49\u001b[0m, \u001b[1;36m54\u001b[0m, \u001b[1;36m233886\u001b[0m\u001b[1m)\u001b[0m,\n",
              "\u001b[2;32m│   │   │   \u001b[0m\u001b[33mstarted_at\u001b[0m=\u001b[1;35mdatetime\u001b[0m\u001b[1;35m.datetime\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m2025\u001b[0m, \u001b[1;36m2\u001b[0m, \u001b[1;36m26\u001b[0m, \u001b[1;36m20\u001b[0m, \u001b[1;36m49\u001b[0m, \u001b[1;36m51\u001b[0m, \u001b[1;36m882957\u001b[0m\u001b[1m)\u001b[0m\n",
              "\u001b[2;32m│   │   \u001b[0m\u001b[1m)\u001b[0m,\n",
              "\u001b[2;32m│   │   \u001b[0m\u001b[1;35mInferenceStep\u001b[0m\u001b[1m(\u001b[0m\n",
              "\u001b[2;32m│   │   │   \u001b[0m\u001b[33mapi_model_response\u001b[0m=\u001b[1;35mCompletionMessage\u001b[0m\u001b[1m(\u001b[0m\n",
              "\u001b[2;32m│   │   │   │   \u001b[0m\u001b[33mcontent\u001b[0m=\u001b[32m\"The\u001b[0m\u001b[32m latest news about Llama-con from Meta in the last week includes the announcement of the LlamaCon conference, which will focus on AI and open-source AI developments. The conference is scheduled for April 29, 2025, and will feature updates on Meta's open-source AI model Llama, as well as new tools and APIs for developers to create apps and products. Additionally, Meta has announced that its Llama model has been downloaded 650 million times in 2024, and the company has maintained 600 million users every day during the year.\"\u001b[0m,\n",
              "\u001b[2;32m│   │   │   │   \u001b[0m\u001b[33mrole\u001b[0m=\u001b[32m'assistant'\u001b[0m,\n",
              "\u001b[2;32m│   │   │   │   \u001b[0m\u001b[33mstop_reason\u001b[0m=\u001b[32m'end_of_turn'\u001b[0m,\n",
              "\u001b[2;32m│   │   │   │   \u001b[0m\u001b[33mtool_calls\u001b[0m=\u001b[1m[\u001b[0m\u001b[1m]\u001b[0m\n",
              "\u001b[2;32m│   │   │   \u001b[0m\u001b[1m)\u001b[0m,\n",
              "\u001b[2;32m│   │   │   \u001b[0m\u001b[33mstep_id\u001b[0m=\u001b[32m'0ee9da65-5a85-409a-84c7-4eb6e9b0b627'\u001b[0m,\n",
              "\u001b[2;32m│   │   │   \u001b[0m\u001b[33mstep_type\u001b[0m=\u001b[32m'inference'\u001b[0m,\n",
              "\u001b[2;32m│   │   │   \u001b[0m\u001b[33mturn_id\u001b[0m=\u001b[32m'de2227df-76e6-4549-a0ee-51bbfc6de046'\u001b[0m,\n",
              "\u001b[2;32m│   │   │   \u001b[0m\u001b[33mcompleted_at\u001b[0m=\u001b[1;35mdatetime\u001b[0m\u001b[1;35m.datetime\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m2025\u001b[0m, \u001b[1;36m2\u001b[0m, \u001b[1;36m26\u001b[0m, \u001b[1;36m20\u001b[0m, \u001b[1;36m49\u001b[0m, \u001b[1;36m56\u001b[0m, \u001b[1;36m97426\u001b[0m\u001b[1m)\u001b[0m,\n",
              "\u001b[2;32m│   │   │   \u001b[0m\u001b[33mstarted_at\u001b[0m=\u001b[1;35mdatetime\u001b[0m\u001b[1;35m.datetime\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m2025\u001b[0m, \u001b[1;36m2\u001b[0m, \u001b[1;36m26\u001b[0m, \u001b[1;36m20\u001b[0m, \u001b[1;36m49\u001b[0m, \u001b[1;36m54\u001b[0m, \u001b[1;36m234624\u001b[0m\u001b[1m)\u001b[0m\n",
              "\u001b[2;32m│   │   \u001b[0m\u001b[1m)\u001b[0m\n",
              "\u001b[2;32m│   \u001b[0m\u001b[1m]\u001b[0m,\n",
              "\u001b[2;32m│   \u001b[0m\u001b[33mturn_id\u001b[0m=\u001b[32m'de2227df-76e6-4549-a0ee-51bbfc6de046'\u001b[0m,\n",
              "\u001b[2;32m│   \u001b[0m\u001b[33mcompleted_at\u001b[0m=\u001b[1;35mdatetime\u001b[0m\u001b[1;35m.datetime\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m2025\u001b[0m, \u001b[1;36m2\u001b[0m, \u001b[1;36m26\u001b[0m, \u001b[1;36m20\u001b[0m, \u001b[1;36m49\u001b[0m, \u001b[1;36m56\u001b[0m, \u001b[1;36m98209\u001b[0m\u001b[1m)\u001b[0m,\n",
              "\u001b[2;32m│   \u001b[0m\u001b[33moutput_attachments\u001b[0m=\u001b[1m[\u001b[0m\u001b[1m]\u001b[0m\n",
              "\u001b[1m)\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Turn</span><span style=\"font-weight: bold\">(</span>\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #808000; text-decoration-color: #808000\">input_messages</span>=<span style=\"font-weight: bold\">[</span>\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">UserMessage</span><span style=\"font-weight: bold\">(</span>\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'What are the latest news about llama-con from Meta in the last week ?'</span>,\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">role</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'user'</span>,\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">context</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"font-weight: bold\">)</span>\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"font-weight: bold\">]</span>,\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #808000; text-decoration-color: #808000\">output_message</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">CompletionMessage</span><span style=\"font-weight: bold\">(</span>\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">\"The latest news about Llama-con from Meta in the last week includes the announcement of the LlamaCon conference, which will focus on AI and open-source AI developments. The conference is scheduled for April 29, 2025, and will feature updates on Meta's open-source AI model Llama, as well as new tools and APIs for developers to create apps and products. Additionally, Meta has announced that its Llama model has been downloaded 650 million times in 2024, and the company has maintained 600 million users every day during the year.\"</span>,\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">role</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'assistant'</span>,\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">stop_reason</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'end_of_turn'</span>,\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">tool_calls</span>=<span style=\"font-weight: bold\">[]</span>\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"font-weight: bold\">)</span>,\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #808000; text-decoration-color: #808000\">session_id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'098c983c-9178-4d08-ad66-767985466f01'</span>,\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #808000; text-decoration-color: #808000\">started_at</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">datetime</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">.datetime</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">20</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">49</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">50</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">969864</span><span style=\"font-weight: bold\">)</span>,\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #808000; text-decoration-color: #808000\">steps</span>=<span style=\"font-weight: bold\">[</span>\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">InferenceStep</span><span style=\"font-weight: bold\">(</span>\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">api_model_response</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">CompletionMessage</span><span style=\"font-weight: bold\">(</span>\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">''</span>,\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">role</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'assistant'</span>,\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">stop_reason</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'end_of_turn'</span>,\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">tool_calls</span>=<span style=\"font-weight: bold\">[</span>\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   │   │   </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ToolCall</span><span style=\"font-weight: bold\">(</span>\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   │   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">arguments</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'query'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Llama-con Meta latest news last week'</span><span style=\"font-weight: bold\">}</span>,\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   │   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">call_id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'367a78af-c9d7-46d8-83fd-6b96dbc43b50'</span>,\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   │   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">tool_name</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'brave_search'</span>\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   │   │   </span><span style=\"font-weight: bold\">)</span>\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   │   </span><span style=\"font-weight: bold\">]</span>\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"font-weight: bold\">)</span>,\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">step_id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'99c13bb9-d57f-4547-903d-3ecef7983419'</span>,\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">step_type</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'inference'</span>,\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">turn_id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'de2227df-76e6-4549-a0ee-51bbfc6de046'</span>,\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">completed_at</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">datetime</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">.datetime</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">20</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">49</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">51</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">881539</span><span style=\"font-weight: bold\">)</span>,\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">started_at</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">datetime</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">.datetime</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">20</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">49</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">50</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">972186</span><span style=\"font-weight: bold\">)</span>\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"font-weight: bold\">)</span>,\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ToolExecutionStep</span><span style=\"font-weight: bold\">(</span>\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">step_id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'476ebdd8-149f-4774-aad8-25f93c91f8c3'</span>,\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">step_type</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'tool_execution'</span>,\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">tool_calls</span>=<span style=\"font-weight: bold\">[</span>\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   │   </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ToolCall</span><span style=\"font-weight: bold\">(</span>\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">arguments</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'query'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Llama-con Meta latest news last week'</span><span style=\"font-weight: bold\">}</span>,\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">call_id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'367a78af-c9d7-46d8-83fd-6b96dbc43b50'</span>,\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">tool_name</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'brave_search'</span>\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   │   </span><span style=\"font-weight: bold\">)</span>\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"font-weight: bold\">]</span>,\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">tool_responses</span>=<span style=\"font-weight: bold\">[</span>\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   │   </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ToolResponse</span><span style=\"font-weight: bold\">(</span>\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">call_id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'367a78af-c9d7-46d8-83fd-6b96dbc43b50'</span>,\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'{\"query\": \"Llama-con Meta latest news last week\", \"top_k\": [{\"title\": \"Meta\\'s new \\'LlamaCon\\' event will focus on AI | The Verge\", \"url\": \"https://www.theverge.com/news/614455/meta-llamacon-connect-2025-date-announcement\", \"content\": \"Meta\\\\u2019s new \\\\u2018LlamaCon\\\\u2019 event will focus on AI | The Verge Meta\\\\u2019s new \\\\u2018LlamaCon\\\\u2019 event will focus on AI Meta\\\\u2019s new \\\\u2018LlamaCon\\\\u2019 event will focus on AI Emma Roth is a news writer who covers the streaming wars, consumer tech, crypto, social media, and much more. Meta is holding a new \\\\u201cLlamaCon\\\\u201d conference where it will share updates on \\\\u201copen source AI developments\\\\u201d to help developers create apps and products, the company said on Tuesday. Meta said it will reveal more information about LlamaCon, which is named after the company\\\\u2019s open-source AI model Llama, in \\\\u201cthe coming weeks.\\\\u201d The Connect conference will feature the \\\\u201clatest and greatest in Meta Horizon updates\\\\u201d and will \\\\u201cpeel back the curtain on tomorrow\\\\u2019s tech,\\\\u201d Meta says.\", \"score\": 0.8003337, \"raw_content\": null}, {\"title\": \"Meta Announces LlamaCon, Riding the Wave of Llama Model Success\", \"url\": \"https://finance.yahoo.com/news/meta-announces-llamacon-riding-wave-110708834.html\", \"content\": \"Feb 19 - Meta Platforms (META, Financial) announced its upcoming developer conference, LlamaCon, scheduled for April 29, 2025, in response to the significant growth of its open-source Llama model\", \"score\": 0.78512776, \"raw_content\": null}, {\"title\": \"Meta Announces LlamaCon, Riding the Wave of Llama Model Success - GuruFocus\", \"url\": \"https://www.gurufocus.com/news/2703591/meta-announces-llamacon-riding-the-wave-of-llama-model-success\", \"content\": \"Feb 19 - Meta Platforms (META, Financial) announced its upcoming developer conference, LlamaCon, scheduled for April 29, 2025, in response to the significant growth of its open-source Llama model. A total of 650 million times the Llama model was downloaded in 2024; meanwhile, Meta AI maintained 600 million users every day during the year.\", \"score\": 0.783542, \"raw_content\": null}, {\"title\": \"Meta just scheduled a generative AI conference called ... - Engadget\", \"url\": \"https://www.engadget.com/ai/meta-just-scheduled-a-generative-ai-conference-called-llamacon-for-april-29-181351134.html\", \"content\": \"Meta just scheduled a generative AI conference called LlamaCon for April 29 Best in Tech View all Reviews Best Robot Vacuums Best Laptops Best Gaming Laptops Best VPN View all Buying Guides View all Gear View all Entertainment View all Tomorrow The best music streaming services Meta just scheduled a generative AI conference called LlamaCon for April 29 Meta just announced its first-ever LlamaCon, a dev conference dedicated to generative AI. The company titled the event after its family of generative AI models. Meta promises to \\\\u201cshare the latest on our open source AI developments to help developers do what they do best: build amazing apps and products.\\\\u201d Beyond that vague description, we don\\\\u2019t know much.\", \"score\": 0.7451949, \"raw_content\": null}, {\"title\": \"Meta\\'s LlamaCon 2025: What Developers Should Expect - Analytics Insight\", \"url\": \"https://www.analyticsinsight.net/ampstories/tech-news/metas-llamacon-2025-what-developers-should-expect\", \"content\": \"Meta\\'s LlamaCon 2025 will showcase cutting-edge AI advancements, including updates to Llama models for more efficient and scalable applications. Developers can expect new tools and APIs that enhance AI-driven chatbots, automation, and content creation across Meta\\'s platforms.\", \"score\": 0.71948266, \"raw_content\": null}]}'</span>,\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">tool_name</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'brave_search'</span>,\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">metadata</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   │   </span><span style=\"font-weight: bold\">)</span>\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"font-weight: bold\">]</span>,\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">turn_id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'de2227df-76e6-4549-a0ee-51bbfc6de046'</span>,\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">completed_at</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">datetime</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">.datetime</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">20</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">49</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">54</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">233886</span><span style=\"font-weight: bold\">)</span>,\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">started_at</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">datetime</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">.datetime</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">20</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">49</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">51</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">882957</span><span style=\"font-weight: bold\">)</span>\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"font-weight: bold\">)</span>,\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">InferenceStep</span><span style=\"font-weight: bold\">(</span>\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">api_model_response</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">CompletionMessage</span><span style=\"font-weight: bold\">(</span>\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">\"The latest news about Llama-con from Meta in the last week includes the announcement of the LlamaCon conference, which will focus on AI and open-source AI developments. The conference is scheduled for April 29, 2025, and will feature updates on Meta's open-source AI model Llama, as well as new tools and APIs for developers to create apps and products. Additionally, Meta has announced that its Llama model has been downloaded 650 million times in 2024, and the company has maintained 600 million users every day during the year.\"</span>,\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">role</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'assistant'</span>,\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">stop_reason</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'end_of_turn'</span>,\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">tool_calls</span>=<span style=\"font-weight: bold\">[]</span>\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"font-weight: bold\">)</span>,\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">step_id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'0ee9da65-5a85-409a-84c7-4eb6e9b0b627'</span>,\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">step_type</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'inference'</span>,\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">turn_id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'de2227df-76e6-4549-a0ee-51bbfc6de046'</span>,\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">completed_at</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">datetime</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">.datetime</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">20</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">49</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">56</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">97426</span><span style=\"font-weight: bold\">)</span>,\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">started_at</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">datetime</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">.datetime</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">20</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">49</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">54</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">234624</span><span style=\"font-weight: bold\">)</span>\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"font-weight: bold\">)</span>\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"font-weight: bold\">]</span>,\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #808000; text-decoration-color: #808000\">turn_id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'de2227df-76e6-4549-a0ee-51bbfc6de046'</span>,\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #808000; text-decoration-color: #808000\">completed_at</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">datetime</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">.datetime</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">20</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">49</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">56</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">98209</span><span style=\"font-weight: bold\">)</span>,\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #808000; text-decoration-color: #808000\">output_attachments</span>=<span style=\"font-weight: bold\">[]</span>\n",
              "<span style=\"font-weight: bold\">)</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluating Agents\n",
        "\n",
        "Since this agentic loop is controlled by the llm. We need some ways to evaluate this agent to know if this is working properly for our use case.\n",
        "\n",
        "The above prompt was fairly simple but what if there were more complex prompts.\n",
        "\n",
        "For eg. What was the first film of the director of the movie that was released in Novemeber of 2014 and was about space time travel.\n",
        "\n",
        "Agent Evals can help you understand and quantify the agent quality by checking outputs against a representative dataset.\n",
        "\n",
        "To test agents for ability to websearch, we will use the SimpleQA dataset.\n",
        "\n"
      ],
      "metadata": {
        "id": "LOe8ciwrml4l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defining a eval dataset"
      ],
      "metadata": {
        "id": "6-Ls1Aecplvy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lets register a dataset with llama-stack so that we can use it for eval purposes across different agents\n",
        "\n",
        "# we will use the /datasets api for this.\n",
        "dataset_id = \"huggingface::simpleqa\"  # has 4.3k qustion / answer pairs\n",
        "client.datasets.register(\n",
        "    dataset_id=dataset_id,\n",
        "    # The data is available on hugging face ( this could be local file system too )\n",
        "    provider_id=\"huggingface\",\n",
        "    url={\"uri\": \"https://huggingface.co/datasets/llamastack/evals\"},\n",
        "    metadata={\n",
        "        \"path\": \"llamastack/evals\",\n",
        "        \"name\": \"evals__simpleqa\",\n",
        "        \"split\": \"train\",\n",
        "    },\n",
        "    dataset_schema={\n",
        "        \"input_query\": {\"type\": \"string\"},\n",
        "        \"expected_answer\": {\"type\": \"string\"},\n",
        "        \"chat_completion_input\": {\"type\": \"chat_completion_input\"},\n",
        "    }\n",
        ")\n",
        "\n",
        "eval_rows = client.datasetio.get_rows_paginated(\n",
        "    dataset_id=dataset_id,\n",
        "    rows_in_page=20,\n",
        ")\n",
        "\n",
        "for row in eval_rows.rows[:1]:\n",
        "    pprint({'input_query': row['input_query'], 'expected_answer': row['expected_answer']})\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "id": "8mvOy5CRmkpY",
        "outputId": "dc9e2313-212b-4eab-be49-4d334d7eefb7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m{\u001b[0m\u001b[32m'input_query'\u001b[0m: \u001b[32m'Who received the IEEE Frank Rosenblatt Award in 2010?'\u001b[0m, \u001b[32m'expected_answer'\u001b[0m: \u001b[32m'Michio Sugeno'\u001b[0m\u001b[1m}\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'input_query'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Who received the IEEE Frank Rosenblatt Award in 2010?'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'expected_answer'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Michio Sugeno'</span><span style=\"font-weight: bold\">}</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defining a Scoring function\n",
        "\n",
        "Scoring functions are what tranlates the output answer into a number so that we can quantify about it and aggregate the evals at the entire dataset level."
      ],
      "metadata": {
        "id": "RyMJxeuuqlPO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We can use the scoring_functions.list api to know all available scoring functions,\n",
        "# We also allow for adding new scoring functions\n",
        "\n",
        "scoring_params = {}\n",
        "for sf in client.scoring_functions.list()[:3]:\n",
        "  scoring_params[sf.identifier] = sf.params\n",
        "  print(f\"{sf.identifier} --> {sf.description}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M4P8iVuPrVdc",
        "outputId": "aab77697-fa34-4001-f134-bc88b50971ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "basic::equality --> Returns 1.0 if the input is equal to the target, 0.0 otherwise.\n",
            "basic::regex_parser_multiple_choice_answer --> Extract answer from response matching Answer: [the_answer_letter], and compare with expected result\n",
            "basic::subset_of --> Returns 1.0 if the expected is included in generated, 0.0 otherwise.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scoring_function = \"basic::subset_of\"\n",
        "\n",
        "response = client.scoring.score(\n",
        "    input_rows=[\n",
        "        {\n",
        "            \"input_query\": \"What is the capital of France?\",\n",
        "            \"expected_answer\": \"Paris\",\n",
        "            \"generated_answer\": \"Paris is the capital of France.\"\n",
        "        },\n",
        "        {\n",
        "            \"input_query\": \"Where is the 2026 fifa world cup ?\",\n",
        "            \"expected_answer\": \"North America\",\n",
        "            \"generated_answer\": \"USA\",\n",
        "        }\n",
        "    ],\n",
        "    scoring_functions={\n",
        "        scoring_function: scoring_params[scoring_function]\n",
        "    }\n",
        ")\n",
        "\n",
        "pprint(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        },
        "id": "QvD6MilJq2jp",
        "outputId": "a0f4fc4f-af61-4e1a-9411-d2b388973fa6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;35mScoringScoreResponse\u001b[0m\u001b[1m(\u001b[0m\n",
              "\u001b[2;32m│   \u001b[0m\u001b[33mresults\u001b[0m=\u001b[1m{\u001b[0m\n",
              "\u001b[2;32m│   │   \u001b[0m\u001b[32m'basic::subset_of'\u001b[0m: \u001b[1;35mScoringResult\u001b[0m\u001b[1m(\u001b[0m\n",
              "\u001b[2;32m│   │   │   \u001b[0m\u001b[33maggregated_results\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'accuracy'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'accuracy'\u001b[0m: \u001b[1;36m0.5\u001b[0m, \u001b[32m'num_correct'\u001b[0m: \u001b[1;36m1.0\u001b[0m, \u001b[32m'num_total'\u001b[0m: \u001b[1;36m2\u001b[0m\u001b[1m}\u001b[0m\u001b[1m}\u001b[0m,\n",
              "\u001b[2;32m│   │   │   \u001b[0m\u001b[33mscore_rows\u001b[0m=\u001b[1m[\u001b[0m\u001b[1m{\u001b[0m\u001b[32m'score'\u001b[0m: \u001b[1;36m1.0\u001b[0m\u001b[1m}\u001b[0m, \u001b[1m{\u001b[0m\u001b[32m'score'\u001b[0m: \u001b[1;36m0.0\u001b[0m\u001b[1m}\u001b[0m\u001b[1m]\u001b[0m\n",
              "\u001b[2;32m│   │   \u001b[0m\u001b[1m)\u001b[0m\n",
              "\u001b[2;32m│   \u001b[0m\u001b[1m}\u001b[0m\n",
              "\u001b[1m)\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ScoringScoreResponse</span><span style=\"font-weight: bold\">(</span>\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #808000; text-decoration-color: #808000\">results</span>=<span style=\"font-weight: bold\">{</span>\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"color: #008000; text-decoration-color: #008000\">'basic::subset_of'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ScoringResult</span><span style=\"font-weight: bold\">(</span>\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">aggregated_results</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'accuracy'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'accuracy'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.5</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'num_correct'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.0</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'num_total'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">}}</span>,\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">score_rows</span>=<span style=\"font-weight: bold\">[{</span><span style=\"color: #008000; text-decoration-color: #008000\">'score'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.0</span><span style=\"font-weight: bold\">}</span>, <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'score'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0</span><span style=\"font-weight: bold\">}]</span>\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"font-weight: bold\">)</span>\n",
              "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"font-weight: bold\">}</span>\n",
              "<span style=\"font-weight: bold\">)</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluating Agents\n",
        "\n",
        "Now that we have finalized the dataset and the scoring function, lets write a few agents and evaluate them."
      ],
      "metadata": {
        "id": "__-uxzwDr6jK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import time\n",
        "import uuid\n",
        "\n",
        "from pydantic import BaseModel\n",
        "from typing import Any, Dict, Optional\n",
        "\n",
        "from llama_stack_client.types.agents import Turn\n",
        "\n",
        "\n",
        "class EvalRow(BaseModel):\n",
        "    input_query: str\n",
        "    expected_answer: str\n",
        "    turn: Optional[Turn]\n",
        "    scores: Dict[str, Any]\n",
        "\n",
        "    @property\n",
        "    def generated_answer(self) -> str:\n",
        "        if self.turn:\n",
        "            return self.turn.output_message.content\n",
        "        else:\n",
        "            return None\n",
        "\n",
        "def tool_call_fraction(eval_responses) -> bool:\n",
        "    has_tools = 0\n",
        "    for r in eval_responses:\n",
        "        if r.turn is not None:\n",
        "          has_tools += int(any(step.step_type == \"tool_execution\" for step in r.turn.steps))\n",
        "\n",
        "    return 0 if len(eval_responses) == 0 else has_tools * 1.0 / len(eval_responses)\n",
        "\n",
        "\n",
        "def run_generate(client, agent, eval_rows, sleep_time=3):\n",
        "  \"\"\"\n",
        "  Run the agent on each of the eval_rows and produce the answers.\n",
        "  \"\"\"\n",
        "  turns = []\n",
        "  for r in eval_rows.rows:\n",
        "    session_id = agent.create_session(uuid.uuid4().hex)\n",
        "    try:\n",
        "      turn = agent.create_turn(\n",
        "          messages=[\n",
        "              {\n",
        "                  \"role\": \"user\",\n",
        "                  \"content\": r[\"input_query\"],\n",
        "              }\n",
        "          ],\n",
        "          session_id=session_id,\n",
        "          stream=False,\n",
        "      )\n",
        "      turns.append(turn)\n",
        "      time.sleep(sleep_time)\n",
        "    except Exception as e:\n",
        "      answer = f\"Exception: Failed to generate answer with error: {e}\"\n",
        "      turns.append(None)\n",
        "      continue\n",
        "\n",
        "  return turns\n",
        "\n",
        "\n",
        "def run_eval(client, agent, eval_rows, scoring_functions):\n",
        "  \"\"\"\n",
        "  Run the agent on each of the eval_rows and produce the answers.\n",
        "  Score the answers by comparing to the expected answers.\n",
        "  Return eval results, scores and aggregated metrics.\n",
        "  \"\"\"\n",
        "  # get correct scoring params for provided scoring_functions\n",
        "  scoring_params = {}\n",
        "  for sf in client.scoring_functions.list():\n",
        "    scoring_params[sf.identifier] = sf.params\n",
        "\n",
        "  # generate responses for agents\n",
        "  turns = run_generate(client, agent, eval_rows)\n",
        "\n",
        "  # score\n",
        "  input_rows = []\n",
        "  for turn, row in zip(turns, eval_rows.rows):\n",
        "    if turn is None:\n",
        "      answer = \"Failed to generate an answer\"\n",
        "    else:\n",
        "      answer = turn.output_message.content\n",
        "    input_rows.append({\n",
        "        \"input_query\": row[\"input_query\"],\n",
        "        \"expected_answer\": row[\"expected_answer\"],\n",
        "        \"generated_answer\": answer\n",
        "    })\n",
        "  scoring_function_params = {\n",
        "      sf_id: scoring_params[sf_id] for sf_id in scoring_functions\n",
        "  }\n",
        "  response = client.scoring.score(\n",
        "      input_rows=input_rows,\n",
        "      scoring_functions=scoring_function_params\n",
        "  )\n",
        "  eval_responses = []\n",
        "  aggregated_eval_results = {}\n",
        "  eval_scores = [{} for _ in range(len(input_rows))]  # [{}, {} ... accumulating all scores for each row]\n",
        "  for scoring_fn, result in response.results.items():\n",
        "      for i, score in enumerate(result.score_rows):\n",
        "          eval_scores[i][scoring_fn] = score[\"score\"]\n",
        "      aggregated_eval_results[scoring_fn] = result.aggregated_results\n",
        "\n",
        "  for turn, ip_row, scores in zip(turns, input_rows, eval_scores):\n",
        "      eval_responses.append(\n",
        "          EvalRow(\n",
        "              input_query=ip_row[\"input_query\"],\n",
        "              expected_answer=ip_row[\"expected_answer\"],\n",
        "              turn=turn,\n",
        "              scores=scores,\n",
        "          )\n",
        "      )\n",
        "\n",
        "  return eval_responses, aggregated_eval_results\n"
      ],
      "metadata": {
        "id": "2ekb8boksnXd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## First Agent\n",
        "\n",
        "- We will start by defining a simple agent to begin with.\n",
        "- The agent has access to a web-search tool (we use the tavily provider for getting search results).\n",
        "- We start with the Llama 3.1 70B model\n"
      ],
      "metadata": {
        "id": "rpzb_qMLDhZv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "agent1 = Agent(\n",
        "    client,\n",
        "    AgentConfig(\n",
        "        model=\"meta-llama/Llama-3.1-70B-Instruct\",\n",
        "        instructions=\"You are a helpful assistant\",\n",
        "        toolgroups=[\"builtin::websearch\"],\n",
        "    )\n",
        ")\n",
        "\n",
        "print(f\"Evaluating for agent: {agent1.agent_id}\")\n",
        "responses, metrics = run_eval(client, agent1, eval_rows, [\"basic::subset_of\"])\n",
        "# pprint(responses[:2])\n",
        "print(f\"Metrics: {metrics}\")\n",
        "print(f\"Tool call fraction: {tool_call_fraction(responses)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_38iaz3Pss5x",
        "outputId": "67a9bacd-ea83-4e50-e0a2-9298e6b6b7e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating for agent: 740186ba-86eb-4874-90ab-45984a702301\n",
            "Metrics: {'basic::subset_of': {'accuracy': {'accuracy': 0.25, 'num_correct': 5.0, 'num_total': 20}}}\n",
            "Tool call fraction: 0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As seen,\n",
        "- the agent got correct answers for a few of the questions.\n",
        "- only a small fraction of the requests actually resulted in a tool call.\n",
        "\n",
        "Lets iterate a bit more on the prompt and may be use a better model to see if we can get better performance."
      ],
      "metadata": {
        "id": "UcJIbAgwD_uZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Trying the Llama 3.3 70B model\n",
        "\n",
        "There are two issues that one can see in the previous results,\n",
        "- The web_search tool is not called on several rows\n",
        "- The answer tends to be more verbose sentences\n",
        "\n",
        "Lets try a better model in Llama3.3 to see if it can do better tool calling.\n",
        "\n",
        "Also note the update to the instructions explicitly suggesting use of the web search tool."
      ],
      "metadata": {
        "id": "6Nnea59lUEFZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "agent2 = Agent(\n",
        "    client,\n",
        "    AgentConfig(\n",
        "        model=\"meta-llama/Llama-3.3-70B-Instruct\",\n",
        "        instructions=\"\"\"\n",
        "        Additional Instructions:\n",
        "        - When provided with questions, use the wesearch tool if necessary.\n",
        "        \"\"\",\n",
        "        toolgroups=[\"builtin::websearch\"],\n",
        "        enable_session_persistence=True,\n",
        "    )\n",
        ")\n",
        "\n",
        "print(f\"Evaluating for agent: {agent2.agent_id}\")\n",
        "responses, metrics = run_eval(client, agent2, eval_rows, [\"basic::subset_of\"])\n",
        "# pprint(responses[:2])\n",
        "print(f\"Metrics: {metrics}\")\n",
        "print(f\"Tool call fraction: {tool_call_fraction(responses)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6tWElSGSsxva",
        "outputId": "2f3c33aa-b392-407e-9f8d-3fba2def46d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating for agent: a11d9286-c38e-4e71-9071-5b1ddbe272a8\n",
            "Metrics: {'basic::subset_of': {'accuracy': {'accuracy': 0.7, 'num_correct': 14.0, 'num_total': 20}}}\n",
            "Tool call fraction: 0.45\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "- These changes definitely helped, we were able to push both accuracy and tool invocations went up.\n",
        "- Looks like the model has much better knowledge and was able to answer a lot of questions without web search.\n"
      ],
      "metadata": {
        "id": "E4gSRLLUEl2F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ReACT Agent\n",
        "\n",
        "ReACT stands for \"Reasoning and Acting.\" It's a framework that combines reasoning and acting in an interleaved manner to improve problem-solving capabilities of AI agents.\n",
        "\n",
        "The ReACT framework enables agents to:\n",
        "- Reason about a problem by breaking it down into steps\n",
        "- Act by taking actions (tool calls) based on that reasoning\n",
        "- Observe the results of those actions\n",
        "- Update their reasoning based on observations\n",
        "\n",
        "This cycle of Reason-Act-Observe creates a more effective problem-solving approach that helps LLMs tackle complex tasks that require multiple steps and adaptation based on intermediate results\n",
        "\n",
        "We provide out of the box utility in llama stack to enable agents work with the ReACT framework.\n",
        "\n",
        "Here is a sample of what we expect agent to return,\n",
        "```\n",
        "You must always respond in the following JSON format:\n",
        "{\n",
        "    \"thought\": $THOUGHT_PROCESS,\n",
        "    \"action\": {\n",
        "        \"tool_name\": $TOOL_NAME,\n",
        "        \"tool_params\": $TOOL_PARAMS\n",
        "    },\n",
        "    \"answer\": $ANSWER\n",
        "}\n",
        "```\n",
        "- Basically, we are pushing the model to think before taking actions\n",
        "- Separately, we also use structured outputs to force the model to produce results of the type `ReActOutput` so that its easy to parse the outputs."
      ],
      "metadata": {
        "id": "OjNSQwjeZ2rX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lets look at the prompt\n",
        "\n",
        "def build_prompt(builtin_toolgroups, additonal_instructions=None):\n",
        "    tool_defs = []\n",
        "    for x in builtin_toolgroups:\n",
        "        tool_defs.extend(\n",
        "            [\n",
        "                {\n",
        "                    \"name\": tool.identifier,\n",
        "                    \"description\": tool.description,\n",
        "                    \"parameters\": tool.parameters,\n",
        "                }\n",
        "                for tool in client.tools.list(toolgroup_id=x)\n",
        "            ]\n",
        "        )\n",
        "    tool_names = \", \".join([x[\"name\"] for x in tool_defs])\n",
        "    tool_descriptions = \"\\n\".join([f\"- {x['name']}: {x}\" for x in tool_defs])\n",
        "\n",
        "    instruction = DEFAULT_REACT_AGENT_SYSTEM_PROMPT_TEMPLATE.replace(\n",
        "        \"<<tool_names>>\", tool_names\n",
        "    ).replace(\n",
        "        \"<<tool_descriptions>>\", tool_descriptions\n",
        "    )\n",
        "    if additonal_instructions:\n",
        "      instruction += f\"\\n\\n{additional_instructions}\"\n",
        "\n",
        "    return instruction\n",
        "\n",
        "instruction = build_prompt(\n",
        "    [\"builtin::websearch\"],\n",
        "    \"\"\"\n",
        "    Additional Instructions:\n",
        "    - Respond concisely with just the answer and not long sentences.\n",
        "    \"\"\"\n",
        ")\n",
        "print(instruction[:2500])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zw_aHFWWU0al",
        "outputId": "65cb4cd4-1a01-4cf0-cd6a-8eb46dd156dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "You are an expert assistant who can solve any task using tool calls. You will be given a task to solve as best you can.\n",
            "To do so, you have been given access to the following tools: web_search\n",
            "\n",
            "You must always respond in the following JSON format:\n",
            "{\n",
            "    \"thought\": $THOUGHT_PROCESS,\n",
            "    \"action\": {\n",
            "        \"tool_name\": $TOOL_NAME,\n",
            "        \"tool_params\": $TOOL_PARAMS\n",
            "    },\n",
            "    \"answer\": $ANSWER\n",
            "}\n",
            "\n",
            "Specifically, this json should have a `thought` key, a `action` key and an `answer` key.\n",
            "\n",
            "The `action` key should specify the $TOOL_NAME the name of the tool to use and the `tool_params` key should specify the parameters key as input to the tool.\n",
            "\n",
            "Make sure to have the $TOOL_PARAMS as a dictionary in the right format for the tool you are using, and do not put variable names as input if you can find the right values.\n",
            "\n",
            "You should always think about one action to take, and have the `thought` key contain your thought process about this action.\n",
            "If the tool responds, the tool will return an observation containing result of the action. \n",
            "... (this Thought/Action/Observation can repeat N times, you should take several steps when needed. The action key must only use a SINGLE tool at a time.)\n",
            "\n",
            "You can use the result of the previous action as input for the next action.\n",
            "The observation will always be the response from calling the tool: it can represent a file, like \"image_1.jpg\". You do not need to generate them, it will be provided to you. \n",
            "Then you can use it as input for the next action. You can do it for instance as follows:\n",
            "\n",
            "Observation: \"image_1.jpg\"\n",
            "{\n",
            "    \"thought\": \"I need to transform the image that I received in the previous observation to make it green.\",\n",
            "    \"action\": {\n",
            "        \"tool_name\": \"image_transformer\",\n",
            "        \"tool_params\": {\"image\": \"image_1.jpg\"}\n",
            "    },\n",
            "    \"answer\": null\n",
            "}\n",
            "\n",
            "\n",
            "To provide the final answer to the task, use the `answer` key. It is the only way to complete the task, else you will be stuck on a loop. So your final output should look like this:\n",
            "Observation: \"your observation\"\n",
            "\n",
            "{\n",
            "    \"thought\": \"you thought process\",\n",
            "    \"action\": null,\n",
            "    \"answer\": \"insert your final answer here\"\n",
            "}\n",
            "\n",
            "Here are a few examples using notional tools:\n",
            "---\n",
            "Task: \"Generate an image of the oldest person in this document.\"\n",
            "\n",
            "Your Response:\n",
            "{\n",
            "    \"thought\": \"I will proceed step by step and use the following tools: `document_qa` to find the oldest person in the document, then `image_generator` to generate an image according to the answer.\",\n",
            "    \"action\": {\n",
            "        \"tool_na\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "agent_config = AgentConfig(\n",
        "    model=\"meta-llama/Llama-3.3-70B-Instruct\",\n",
        "    instructions=instruction,\n",
        "    toolgroups=[\"builtin::websearch\"],\n",
        "    tool_config={\n",
        "        \"system_message_behavior\": \"replace\",\n",
        "    },\n",
        "    response_format={\n",
        "        \"type\": \"json_schema\",\n",
        "        \"json_schema\": ReActOutput.model_json_schema(),\n",
        "    }\n",
        ")\n",
        "\n",
        "agent3 = ReActAgent(\n",
        "    client=client,\n",
        "    model=\"meta-llama/Llama-3.3-70B-Instruct\",\n",
        "    custom_agent_config=agent_config,\n",
        ")\n",
        "\n",
        "responses, metrics = run_eval(client, agent3, eval_rows, [\"basic::subset_of\"])\n",
        "# pprint(responses[:2])\n",
        "print(f\"Metrics: {metrics}\")\n",
        "print(f\"Tool call fraction: {tool_call_fraction(responses)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xdbQXpKku7uv",
        "outputId": "85dbab97-4879-4cf0-e3ad-ec0d18e40d58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Metrics: {'basic::subset_of': {'accuracy': {'accuracy': 0.65, 'num_correct': 13.0, 'num_total': 20}}}\n",
            "Tool call fraction: 0.95\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "While the accuracy is in the same range, we were abe to get much higher tool invocations using this framework."
      ],
      "metadata": {
        "id": "rPSYcCtU2SZ-"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ruFPzQGtW2lJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "qb0KXFCiW-Vn"
      }
    }
  ]
}